{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3098a0ac",
   "metadata": {},
   "source": [
    "## Latin pedagogy tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1aee7",
   "metadata": {},
   "source": [
    "We make use of the CLTK library, a NLP library for classical languages.\n",
    "\n",
    "**Introduction** : https://aclanthology.org/2021.acl-demo.3.pdf\n",
    "\n",
    "**Documentation**\n",
    "* API : https://docs.cltk.org/en/latest/index.html\n",
    "* Demos : https://github.com/cltk/cltk/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fa6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Architecti est scientia pluribus disciplinis et variis eruditionibus ornata, quae ab ceteris artibus perficiuntur.\n",
    "Opera ea nascitur et fabrica et ratiocinatione.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4b5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = get_corpus_reader(corpus_name='latin_text_perseus', language='latin')\n",
    "from cltk.data.fetch import FetchCorpus\n",
    "corpus_downloader = FetchCorpus(language=\"lat\")\n",
    "corpus_downloader.import_corpus('lat_text_perseus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1239f92",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54675bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.alphabet.lat import drop_latin_punctuation\n",
    "import re\n",
    "\n",
    "def cleanDoc(text, convertLower=False):\n",
    "    # Remove metainfo like [c 1Kb]\n",
    "    cleaned = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    # Remove wide spaces\n",
    "    cleaned = cleaned.replace(\"   \", \" \").replace(\"  \", \" \")\n",
    "    return cleaned.lower() if convertLower else cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ed74c",
   "metadata": {},
   "source": [
    "## Macronizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da228339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example : Architecti est scientia pluribus -> architectƒ´ est scientiƒÅ pl≈´ribus\n"
     ]
    }
   ],
   "source": [
    "from cltk.prosody.lat.macronizer import Macronizer\n",
    "\n",
    "# NOTE: subpar accuracy for the macronizer \n",
    "def macronizer(text: str) -> str:\n",
    "    macronizer = Macronizer(\"tag_tnt\")\n",
    "    text = macronizer.macronize_text(text)\n",
    "    return text\n",
    "\n",
    "tƒìxt = macronizer(text)\n",
    "print(f\"Example : {text[1:33]} -> {tƒìxt[:32]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3790c",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce17659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example : ['filias', 'pueri', 'cecini', 'variis'] -> ['filia', 'puer', 'cano', 'varius1']\n"
     ]
    }
   ],
   "source": [
    "from cltk.lemmatize.lat import LatinBackoffLemmatizer\n",
    "\n",
    "# Returns tuples of (original, root)\n",
    "# Requires lower-case, non-macron inputs\n",
    "def lemmatize(tokens: list)-> list:\n",
    "    lemmatizer = LatinBackoffLemmatizer()\n",
    "    tokens = lemmatizer.lemmatize(tokens)\n",
    "    return [root for _, root in tokens]\n",
    "\n",
    "tokens = [\"filias\", \"pueri\", \"cecini\", \"variis\"]\n",
    "print(f\"Example : {tokens} -> {lemmatize(tokens)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c65254f0",
   "metadata": {},
   "source": [
    "## Decliner\n",
    "\n",
    "Declension encodings are described here : https://github.com/cltk/latin_treebank_perseus#readme\n",
    "\n",
    "E.g. --s----n- => singular nominative\n",
    "\n",
    "It can be used to construct declension tables as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a60c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case        Singular    Plural\n",
      "----------  ----------  --------\n",
      "Nominative  le≈ç         le≈çnƒìs\n",
      "Genitive    le≈çnis      le≈çnum\n",
      "Dative      le≈çnƒ´       le≈çnibus\n",
      "Accusative  le≈çnem      le≈çnƒìs\n",
      "Ablative    le≈çne       le≈çnibus\n"
     ]
    }
   ],
   "source": [
    "from cltk.morphology.lat import CollatinusDecliner\n",
    "from collections import OrderedDict\n",
    "from tabulate import tabulate\n",
    "\n",
    "words = ['leo', 'via']\n",
    "def declensions(rootWords: list)-> dict:\n",
    "    dec, decliner = {}, CollatinusDecliner()\n",
    "    for word in rootWords:\n",
    "        # Expect root words only\n",
    "        try: dec[word] = decliner.decline(word)\n",
    "        except Exception: print('Not a root word')\n",
    "    return dec\n",
    "\n",
    "# Usage example : declension table (only nouns for now)\n",
    "def printDecTable(lemma, POS):\n",
    "\n",
    "    rows = []\n",
    "    cases = {'n':'Nominative',\n",
    "             'g':'Genitive',\n",
    "             'd':'Dative',\n",
    "             'a':'Accusative',\n",
    "             'b':'Ablative'}\n",
    "\n",
    "    d = OrderedDict({c: {} for c in cases.keys()})\n",
    "    if POS==\"noun\":\n",
    "        declens = CollatinusDecliner().decline(lemma)\n",
    "        for dec, code in declens:\n",
    "            number, case = code[2], code[7]\n",
    "            if case in d: d[case][number] = macronizer(dec).lower()\n",
    "        \n",
    "        for key, val in d.items():\n",
    "            row =[cases[key]]+list(val.values())\n",
    "            rows.append(row)\n",
    "            \n",
    "    print(tabulate(rows, headers=['Case', 'Singular', 'Plural']))\n",
    "\n",
    "decs = declensions(words)\n",
    "printDecTable(\"leo\", \"noun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a21599",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41995ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.sentence.lat import LatinPunktSentenceTokenizer\n",
    "from cltk.alphabet.text_normalization import remove_non_latin\n",
    "from cltk.tokenizers.lat.lat import LatinWordTokenizer\n",
    "\n",
    "# Sentence tokenizer\n",
    "def sentTokenize(doc: str, punct=True) -> list:\n",
    "    sent_tokenize = LatinPunktSentenceTokenizer()\n",
    "    sentences = sent_tokenize.tokenize(doc)\n",
    "    return [remove_non_latin(s).lower() for s in sentences] if punct else sentences\n",
    "\n",
    "# Word tokenizer\n",
    "def word_Tokenizer(sent: str) -> list:\n",
    "    word_tokenize = LatinWordTokenizer()\n",
    "    tokens = word_tokenize.tokenize(sent)\n",
    "    return tokens\n",
    "\n",
    "sentences = sentTokenize(text)\n",
    "tokens = word_Tokenizer(sentences[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad100471",
   "metadata": {},
   "source": [
    "## NLP pipeline\n",
    "\n",
    "The CLTK library also has a pre-configured NLP pipeline for latin. The most useful feature is that it automatically tokenises the text, processes information (such as gender, case etc.) into each `Word` object and also creates Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc6a6aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.1.6'.\n",
      "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinStanzaProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinLexiconProcess`.\n"
     ]
    }
   ],
   "source": [
    "from cltk import NLP\n",
    "cltk_nlp = NLP(language=\"lat\")\n",
    "\n",
    "text = drop_latin_punctuation(cleanDoc(text, convertLower=True))\n",
    "cltk_doc = cltk_nlp.analyze(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "181e17ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_char_start': None,\n",
       " 'index_char_stop': None,\n",
       " 'index_token': 0,\n",
       " 'index_sentence': 0,\n",
       " 'string': 'architecti',\n",
       " 'pos': verb,\n",
       " 'lemma': 'architico',\n",
       " 'stem': None,\n",
       " 'scansion': None,\n",
       " 'xpos': 'L2|modM|tem4|grp1|casB|gen1',\n",
       " 'upos': 'VERB',\n",
       " 'dependency_relation': 'root',\n",
       " 'governor': -1,\n",
       " 'features': {Aspect: [perfective], Case: [genitive], Degree: [positive], Gender: [masculine], Number: [singular], Tense: [past], VerbForm: [participle], Voice: [passive]},\n",
       " 'category': {F: [neg], N: [neg], V: [pos]},\n",
       " 'embedding': array([ 1.6831e-01, -1.3812e-01, -2.2983e-01, -1.1404e-01,  6.0152e-01,\n",
       "        -4.8938e-02, -4.0779e-01,  7.2283e-01,  1.9987e-01, -8.3087e-02,\n",
       "         2.3405e-01,  2.8201e-01, -2.8227e-01, -4.3663e-01,  1.8110e-01,\n",
       "        -2.1940e-01,  2.9445e-01,  2.3684e-01,  3.9450e-01, -8.0237e-02,\n",
       "        -4.2323e-02,  6.2347e-01,  6.8870e-01,  1.2506e-01, -8.6620e-01,\n",
       "        -3.4647e-01,  4.7634e-01, -3.1648e-01,  5.1305e-01, -6.8620e-01,\n",
       "        -5.4679e-01, -3.2498e-01,  4.9419e-01,  1.5745e-01,  5.1181e-01,\n",
       "        -2.1517e-01, -7.1731e-01,  2.7233e-01, -3.4191e-01,  3.2203e-01,\n",
       "         2.0504e-02, -8.3042e-01,  3.4143e-02,  5.1322e-01, -4.3652e-01,\n",
       "         4.9996e-01, -2.9314e-01, -3.7844e-01, -1.1729e-01, -2.4311e-01,\n",
       "        -4.1604e-01, -4.1774e-01,  8.2656e-02, -1.4123e-01,  1.0930e-01,\n",
       "         2.5166e-01,  2.7034e-01, -1.8688e-01, -8.9835e-02, -5.3617e-02,\n",
       "        -5.2945e-01,  2.2738e-01, -5.4287e-02,  4.4079e-01, -2.1368e-01,\n",
       "        -1.6360e-01, -7.4949e-01, -2.3947e-02,  8.2109e-01,  2.7324e-01,\n",
       "         2.9549e-01,  5.5805e-01,  5.4817e-01, -5.1027e-02, -3.8161e-01,\n",
       "         1.3760e-01,  1.2567e-01, -1.4092e-01,  3.0471e-01,  2.4955e-01,\n",
       "        -1.3885e-01, -4.9916e-02,  1.4680e-01, -7.3494e-01, -2.0065e-01,\n",
       "        -7.5007e-02,  4.4810e-01,  5.0051e-02, -3.5263e-01,  3.5195e-01,\n",
       "         3.4328e-01,  8.4595e-02,  3.2477e-02, -6.5753e-02, -2.4331e-01,\n",
       "        -2.4292e-01,  6.0502e-02,  1.3002e-01, -1.1637e-01, -2.2737e-01,\n",
       "        -1.5002e-01, -2.5103e-01,  3.2530e-01,  5.9283e-01, -4.0175e-02,\n",
       "        -3.2645e-01,  2.3888e-01,  1.2901e-01, -2.3703e-01,  1.7824e-01,\n",
       "        -3.4477e-01, -1.1356e-01,  7.1292e-02,  1.8320e-01,  4.9042e-01,\n",
       "         9.3680e-01, -1.1494e-03, -5.8415e-01, -4.7882e-02, -2.1693e-01,\n",
       "        -8.0725e-02, -2.7788e-01, -9.6801e-02,  2.1233e-01, -7.5234e-02,\n",
       "         8.0030e-01,  3.8897e-01,  2.0040e-01,  3.5076e-02, -9.9692e-02,\n",
       "         3.2927e-01, -1.8435e-01, -5.0573e-02, -2.9967e-01,  6.1460e-01,\n",
       "        -1.4755e-01, -2.3144e-01,  1.7551e-01,  6.0414e-01, -8.9441e-03,\n",
       "        -2.7315e-01, -4.6230e-01, -2.6099e-01,  2.3506e-01, -1.9482e-02,\n",
       "         2.2671e-01,  1.0946e-01, -3.1342e-01, -4.5242e-01,  5.6655e-01,\n",
       "         2.7111e-01, -1.2381e-01,  3.1988e-01, -4.6320e-02,  1.2870e-01,\n",
       "         6.5928e-01, -7.7046e-01,  1.2429e-01, -2.8259e-02,  6.9323e-01,\n",
       "         6.6319e-01,  3.9403e-01, -2.6404e-01,  5.7723e-01, -7.2490e-01,\n",
       "         2.1183e-01, -1.0927e-02, -3.7671e-01, -1.8920e-01, -1.2512e-01,\n",
       "         9.2336e-01,  5.6252e-01,  1.5543e-01,  2.9649e-01,  2.3298e-01,\n",
       "         5.7743e-02, -7.6070e-02,  3.8328e-01,  4.8631e-01, -6.6038e-02,\n",
       "        -3.2890e-01, -1.5572e-01, -2.0214e-01,  4.5119e-01, -4.0250e-01,\n",
       "         1.8195e-01,  2.0836e-01, -3.7422e-01, -2.8373e-01,  5.1923e-01,\n",
       "        -2.1842e-01, -1.0949e-01,  3.1680e-01, -3.9049e-01, -3.1300e-02,\n",
       "        -3.2927e-01, -2.5661e-03, -4.4392e-01,  9.7563e-01,  2.0224e-01,\n",
       "         4.8866e-01, -9.0339e-02,  2.1811e-01, -3.7180e-01, -1.4017e-01,\n",
       "        -5.8437e-01, -8.6539e-02,  2.2871e-01,  9.2594e-03,  2.5886e-01,\n",
       "        -1.5346e-01,  4.7035e-01,  2.7093e-01, -3.7627e-01, -1.4930e-01,\n",
       "        -4.4519e-01,  1.1884e-01,  6.0129e-02, -3.3664e-01, -3.9940e-03,\n",
       "         8.9403e-02,  3.5656e-01,  2.5351e-01,  2.4347e-01,  4.9958e-01,\n",
       "        -4.6188e-01,  2.9846e-01, -4.1477e-01, -3.8869e-01, -1.5090e-01,\n",
       "         1.3962e-01, -1.1737e-01, -2.3727e-01, -1.0290e-01, -4.6437e-01,\n",
       "        -4.8216e-01, -4.1249e-01,  4.4490e-01, -1.7346e-01, -8.0114e-01,\n",
       "         1.6118e-01,  1.5559e-01,  6.0029e-01, -1.0506e-01, -2.6184e-01,\n",
       "         6.5155e-02, -2.4856e-01,  4.2597e-01,  3.9771e-01, -4.8194e-02,\n",
       "        -6.1595e-01, -1.3947e-02, -1.5289e-01, -1.1396e+00, -1.0696e-02,\n",
       "         1.3648e+00, -6.3433e-01, -8.3895e-02, -1.7030e-01,  4.9734e-01,\n",
       "         3.6044e-01, -5.1696e-01,  3.5551e-01, -4.5601e-01,  1.7686e-01,\n",
       "        -1.5582e-01, -3.7384e-02, -5.4505e-02,  3.2110e-01,  2.2699e-02,\n",
       "         2.0379e-01, -1.7513e-01, -2.3534e-01, -1.0456e-01, -2.1578e-01,\n",
       "         2.3636e-01,  1.8826e-02, -1.8292e-01, -3.3703e-01, -4.4438e-01,\n",
       "        -8.3751e-01, -8.9811e-01,  2.2210e-01,  2.3053e-01, -3.8843e-01,\n",
       "        -4.3650e-01, -3.7070e-01,  7.4730e-02,  5.4160e-01,  1.2986e-01,\n",
       "        -3.9765e-01,  9.3646e-02, -9.2133e-01,  4.2413e-01,  2.5478e-01,\n",
       "        -6.3289e-01, -3.3280e-01, -1.3867e-01,  3.5921e-01, -2.5701e-01],\n",
       "       dtype=float32),\n",
       " 'stop': False,\n",
       " 'named_entity': None,\n",
       " 'syllables': None,\n",
       " 'phonetic_transcription': None,\n",
       " 'definition': '',\n",
       " 'stanza_features': 'Aspect=Perf|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cltk_doc.words[0].__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf221a5b",
   "metadata": {},
   "source": [
    "## Dictionary\n",
    "\n",
    "Although the word objects have definitions, the formatting is a mess (one giant string)\n",
    "Use the following API to get a compact definition: https://www.latin-is-simple.com/api/\n",
    "\n",
    "\"intern_type\" can has the following values:\n",
    "- \"dempron\" for demonstrative pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf1e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fabrica, fabricae f. : (1.) craft, art, craft of metalwork/building, construction/building/making (2.) smith's shop, workshop\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Formats the header of the word\n",
    "def formatHeader(header: str, POS: str) -> str:\n",
    "\n",
    "    # Formatting for verbs\n",
    "    if POS==\"verb\":\n",
    "        principalParts = header.split(\",\")\n",
    "        header = \",\".join(principalParts[:1]+principalParts[2:])\n",
    "        return header\n",
    "\n",
    "    # Formatting for nouns\n",
    "    elif POS==\"noun\":\n",
    "        # decParadigm = header[-1]\n",
    "        # Can use the paradigm to further format the header\n",
    "        # E.g. head = formatNoun(header, decParadigm)\n",
    "        header = re.sub(\"[\\[\\]]\", \"\", header)[:-2]\n",
    "        return header\n",
    "    #elif POS==\"adverb\":\n",
    "    #elif POS==\"adjective\":\n",
    "    #elif POS==\"dempron\"\n",
    "    else:\n",
    "        return header\n",
    "\n",
    "def getDefinition(word: str, POS: str) -> str:\n",
    "    word, POS = word.lower(), POS.lower()\n",
    "\n",
    "    apiURL = \"https://www.latin-is-simple.com/api/vocabulary/search/?query=\"+word+\"&forms_only=true\"\n",
    "    r = requests.get(apiURL)\n",
    "\n",
    "    definition = \"\"\n",
    "\n",
    "    # Only one result\n",
    "    if len(r.json())==1:\n",
    "        entry = r.json()[0]\n",
    "        header = formatHeader(entry['full_name'], entry[\"intern_type\"])\n",
    "        body = entry[\"translations_unstructured\"][\"en\"]\n",
    "        definition = header + \" : \" + body\n",
    "    \n",
    "    # Multiple results (get the first entry that matches POS)\n",
    "    else:\n",
    "        for entry in r.json():\n",
    "            if entry[\"intern_type\"]==POS:\n",
    "                header = formatHeader(entry['full_name'], POS)\n",
    "                body = entry[\"translations_unstructured\"][\"en\"]\n",
    "                definition = header + \" : \" + body\n",
    "                break\n",
    "\n",
    "    return definition \n",
    "\n",
    "# Example\n",
    "print(getDefinition('fabrica', 'noun'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f97855e",
   "metadata": {},
   "source": [
    "## USE CASE - generate vocabulary aid\n",
    "\n",
    "I primarily use this library to generate rudimentary word lists (with declension information) to pre-study vocabulary or study as I read.\n",
    "\n",
    "This idea was inspired to automate creating a latin reader like this https://geoffreysteadman.files.wordpress.com/2019/05/ritchie.may2019.pdf\n",
    "\n",
    "It's more useful towards beginners that can have trouble identifying declension forms. \n",
    "\n",
    "The example below makes a vocabulary aid from the Gallic War written by Caesar. More texts are available at https://github.com/cltk/lat_text_latin_library \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "111afdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text : gallia est omnis divisa in partes tres quarum unam incolunt belgae aliam aquitani tertiam qui ipsorum\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Corpus/gall1.txt\", \"r\") as f:\n",
    "    x = f.read()\n",
    "\n",
    "# Get the first paragraph \n",
    "l = re.search(r\"\\[ 1 \\] (.*)\", x)\n",
    "x = cleanDoc(drop_latin_punctuation(l[1]), convertLower=True)\n",
    "\n",
    "print(f\"Example text : {x[:101]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e08fb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallicWars1 = cltk_nlp.analyze(text=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e26d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omnis not found, POS : determiner\n",
      "divisa not found, POS : adjective\n",
      "tres not found, POS : numeral\n",
      "aliam not found, POS : determiner\n",
      "nostra not found, POS : determiner\n",
      "omnes not found, POS : determiner\n",
      "omnium not found, POS : pronoun\n",
      "quod not found, POS : subordinating_conjunction\n",
      "saepe not found, POS : adverb\n",
      "quoque not found, POS : adverb\n",
      "quod not found, POS : subordinating_conjunction\n",
      "fere not found, POS : adposition\n",
      "belgae not found, POS : adjective\n"
     ]
    }
   ],
   "source": [
    "def generateLex(WordList):\n",
    "    s = set()\n",
    "    for word in WordList:\n",
    "        if not word.stop:\n",
    "            result = getDefinition(word.string, str(word.pos))\n",
    "            # Alert if word wasn't found\n",
    "            if result:\n",
    "                s.add(result)\n",
    "            else:\n",
    "                print(word.string + \" not found, POS : \" + str(word.pos))\n",
    "    return s\n",
    "\n",
    "words = generateLex(gallicWars1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0389fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aquitania, Aquitaniae f. : Aquitania, one of the divisions of Gaul/France '\n",
      " '(southwest)',\n",
      " 'Aquitanus/Aquitana/Aquitanum, AO : of Aquitania  (southwest Gaul/France)',\n",
      " 'Belga, Belgae m. : Belgae (pl.)',\n",
      " 'Celtus/Celta/Celtum, AO : Celts',\n",
      " 'Gallia, Galliae f. : Gaul',\n",
      " 'Gallus, Galli m. : Gaul, the Gauls (pl.), cock, rooster',\n",
      " 'Garumna, Garumnae f. : Garonna',\n",
      " 'Helvetia, Helvetiae f. : Switzerland',\n",
      " 'Helvetius, Helvetii m. : Helvetii (pl.), tribe in Central Gaul (Switzerland)',\n",
      " 'Hispania, Hispaniae f. : Hispania, Spain']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "m = sorted(list(words))\n",
    "pprint.pprint(m[:10])\n",
    "with open(\"lexical_list\", \"w\") as f:\n",
    "    f.write(\"\\n\\n\".join(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
